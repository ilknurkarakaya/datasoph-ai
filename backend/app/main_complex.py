"""
DataSoph AI - Enhanced Data Science Assistant
World-class AI with comprehensive data science knowledge and user-controlled file analysis
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict, List, Any
import uvicorn
import logging
import os
# Removed openai import - using pure OpenRouter implementation
import pandas as pd
import numpy as np
import uuid
import asyncio
import json
from datetime import datetime
from dotenv import load_dotenv

# Import our working services only
from app.services.conversation_manager import AdvancedConversationManager, IntentCategory
# from app.services.secure_executor import SecureCodeExecutor  # Disabled for stability
from app.services.data_science_engine import ComprehensiveDataScienceEngine
from app.services.advanced_file_processor import AdvancedFileProcessor
from app.services.universal_file_handler import UniversalFileHandler
from app.services.ocr_processor import OCRProcessor
from app.services.business_intelligence import BusinessIntelligenceEngine, BusinessDomain
from app.services.memory_system import ConversationMemory, MemoryType, ContextImportance
from app.services.expert_data_scientist import ExpertDataAnalyzer, DataScienceConversationAI, get_expert_analyzer, get_conversation_ai

# Import new comprehensive DataSoph AI system
from app.services.datasoph_ai_main import DataSophAI
from app.services.safety_guardrails import PIIDetector, StatisticalGuardrails, ErrorResilienceManager

# Import Smart Response Generator for brilliant multilingual conversations
from app.services.smart_response_generator import smart_response_generator

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create uploads directory
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# Initialize FastAPI app
app = FastAPI(title="DataSoph AI", description="Enhanced Data Science Assistant")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Serve generated charts and reports
from fastapi.staticfiles import StaticFiles
import os

# üîß FIXED: Proper static file serving
import os
from pathlib import Path

# Get absolute paths to static directories
BASE_DIR = Path(__file__).resolve().parent.parent.parent
figures_dir = BASE_DIR / "figures"
reports_dir = BASE_DIR / "reports" 
exports_dir = BASE_DIR / "exports"

# Create directories if they don't exist
figures_dir.mkdir(exist_ok=True)
reports_dir.mkdir(exist_ok=True)
exports_dir.mkdir(exist_ok=True)

# Mount static file directories with absolute paths
app.mount("/static/figures", StaticFiles(directory=str(figures_dir)), name="figures")
app.mount("/static/reports", StaticFiles(directory=str(reports_dir)), name="reports")
app.mount("/static/exports", StaticFiles(directory=str(exports_dir)), name="exports")

logger.info(f"üìÅ Static directories mounted:")
logger.info(f"  - Figures: {figures_dir}")
logger.info(f"  - Reports: {reports_dir}")
logger.info(f"  - Exports: {exports_dir}")

class ChatRequest(BaseModel):
    message: str
    user_id: Optional[str] = None
    file_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    timestamp: str

class UploadResponse(BaseModel):
    file_id: str
    filename: str
    message: str
    size: Optional[int] = None
    type: Optional[str] = None

class FileAnalysisRequest(BaseModel):
    file_id: str
    user_id: Optional[str] = None

class FileContextManager:
    """Manages file context and user sessions"""
    
    def __init__(self):
        self.user_file_sessions: Dict[str, List[Dict]] = {}
        
    def register_file_for_user(self, user_id: str, file_id: str, filename: str) -> None:
        """Register that a user has uploaded a specific file and clear old context"""
        # CLEAR OLD DATA: Reset user's file sessions to prevent confusion with old data
        if user_id in self.user_file_sessions and len(self.user_file_sessions[user_id]) > 0:
            old_count = len(self.user_file_sessions[user_id])
            logger.info(f"üîÑ Clearing {old_count} old file(s) for user {user_id} to prevent data confusion")
        
        # Initialize fresh session for this user
        self.user_file_sessions[user_id] = []
        
        self.user_file_sessions[user_id].append({
            "file_id": file_id,
            "filename": filename,
            "upload_time": datetime.now(),
            "analysis_done": False
        })
        logger.info(f"üìÅ File registered for user {user_id}: {filename} -> {file_id} (fresh context)")
    
    def get_user_files(self, user_id: str) -> List[Dict]:
        """Get all files for a specific user"""
        return self.user_file_sessions.get(user_id, [])
    
    def get_latest_file(self, user_id: str) -> Optional[Dict]:
        """Get the most recently uploaded file for user"""
        files = self.get_user_files(user_id)
        logger.info(f"üîç Getting latest file for user {user_id}, found {len(files)} files")
        if files:
            logger.info(f"üìÑ Latest file: {files[-1]}")
        return files[-1] if files else None
    
    def mark_file_analyzed(self, user_id: str, file_id: str) -> None:
        """Mark a file as analyzed"""
        user_files = self.get_user_files(user_id)
        for file_info in user_files:
            if file_info["file_id"] == file_id:
                file_info["analysis_done"] = True
                break

class EnhancedConversationManager:
    """Enhanced conversation management with warmth and encouragement"""
    
    def __init__(self):
        self.conversation_starters = {
            "Turkish": [
                "Harika bir soru! üòä",
                "√áok g√ºzel bir yakla≈üƒ±m d√º≈ü√ºnm√º≈üs√ºn!",
                "Bu konuda sana yardƒ±mcƒ± olmaktan memnuniyet duyarƒ±m!",
                "Veri biliminde bu ger√ßekten √∂nemli bir konu!",
                "Birlikte bu problemi √ß√∂zelim! üöÄ",
                "M√ºkemmel! Bu tam benim uzmanlƒ±k alanƒ±m! ‚ú®",
                "Bu soruya bayƒ±ldƒ±m! üí°"
            ],
            "English": [
                "That's a fantastic question! üòä",
                "I love this approach you're thinking about!",
                "I'm excited to help you with this!",
                "This is such an important aspect of data science!",
                "Let's dive into this together! üöÄ",
                "Perfect! This is exactly my area of expertise! ‚ú®",
                "I'm thrilled to work on this with you! üí°"
            ]
        }
        
        self.closings = {
            "Turkish": [
                "\n\nüí° Ba≈üka sorularƒ±nƒ± merakla bekliyorum!",
                "\n\nBu konuda daha derinlemesine gitmek istersen, her zaman buradayƒ±m! üòä",
                "\n\nUmarƒ±m bu a√ßƒ±klama faydalƒ± olmu≈ütur. Ba≈üka ne √∂ƒürenmek istersin?",
                "\n\nüöÄ Veri bilimi yolculuƒüunda seni desteklemeye devam edeceƒüim!",
                "\n\n‚ú® Bu analizi geli≈ütirmek i√ßin ba≈üka fikirlerim de var!"
            ],
            "English": [
                "\n\nüí° I'm here for any follow-up questions you might have!",
                "\n\nFeel free to ask if you'd like to dive deeper into any of this! üòä",
                "\n\nI hope this helps! What would you like to explore next?",
                "\n\nüöÄ I'm excited to continue supporting your data science journey!",
                "\n\n‚ú® I have more ideas to enhance this analysis if you're interested!"
            ]
        }
    
    def add_warmth_to_response(self, response: str, language: str) -> str:
        """Add human warmth and encouragement to technical responses"""
        import random
        
        starters = self.conversation_starters[language]
        starter = random.choice(starters)
        
        closings = self.closings[language]
        closing = random.choice(closings)
        
        return f"{starter}\n\n{response}{closing}"

class ComprehensiveDataScienceAI:
    """Enhanced DataSoph AI with world-class data science knowledge"""
    
    def __init__(self, openrouter_api_key: str = None):
        self.logger = logging.getLogger(__name__)
        
        # Initialize OpenRouter client - NO OpenAI dependencies
        from app.services.openrouter_client import create_openrouter_client
        
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        if openrouter_key:
            self.logger.info("üåê Using pure OpenRouter for AI intelligence")
            self.openrouter_client = create_openrouter_client(openrouter_key)
        else:
            self.logger.warning("‚ö†Ô∏è OPENROUTER_API_KEY not found - AI features disabled")
            self.openrouter_client = None
        
        # Initialize enhanced components
        self.conversation_manager = AdvancedConversationManager()
        self.enhanced_conversation = EnhancedConversationManager()
        self.file_manager = FileContextManager()
        self.data_science_engine = ComprehensiveDataScienceEngine()
        self.file_processor = AdvancedFileProcessor()
        self.universal_file_handler = UniversalFileHandler()
        
        # Initialize Expert Data Scientist AI
        self.expert_analyzer = get_expert_analyzer()
        self.expert_conversation_ai = get_conversation_ai()
        
        # Check OCR availability for enhanced prompts
        self.ocr_available = hasattr(self.universal_file_handler, 'ocr_available') and self.universal_file_handler.ocr_available
        self.business_intelligence = BusinessIntelligenceEngine()
        self.memory_system = ConversationMemory()
        
        # Enhanced model selection for different tasks
        self.model_config = {
            "casual": "anthropic/claude-3.5-haiku",
            "technical": "anthropic/claude-3.5-sonnet",
            "coding": "anthropic/claude-3.5-sonnet",
            "analysis": "openai/gpt-4o-mini"
        }
        
        self.logger.info("üöÄ Enhanced DataSoph AI with world-class knowledge initialized!")

    async def process_message(self, message: str, user_id: str, file_id: Optional[str] = None) -> str:
        """Process message with enhanced AI capabilities and file context awareness"""
        
        try:
            # Detect intent and language
            intent, language, confidence = self.conversation_manager.detect_intent_and_language(message, user_id)
            context = self.conversation_manager.get_or_create_context(user_id)
            
            self.logger.info(f"üéØ Intent: {intent.value}, Language: {language}, Confidence: {confidence:.2f}")
            
            # Check if user has uploaded files recently (file context awareness)
            if not file_id:
                latest_file = self.file_manager.get_latest_file(user_id)
                if latest_file:
                    file_id = latest_file["file_id"]
                    self.logger.info(f"üîó Using latest uploaded file: {latest_file['filename']}")
            
            # Enhanced response for casual chat
            if intent == IntentCategory.GREETING and len(message.split()) <= 3 and not file_id:
                return self._generate_enhanced_greeting(language)
            
            # File analysis if provided
            file_analysis = ""
            if file_id:
                file_analysis = await self._analyze_file_advanced(file_id, language)
                
                # If file analysis is empty, try to read from generated EDA report
                if not file_analysis:
                    try:
                        report_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "reports", "REPORT.md")
                        summary_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "reports", "summary.json")
                        
                        if os.path.exists(report_path):
                            with open(report_path, 'r', encoding='utf-8') as f:
                                report_content = f.read()
                            
                            if os.path.exists(summary_path):
                                with open(summary_path, 'r', encoding='utf-8') as f:
                                    summary_data = json.load(f)
                                
                                if language == "Turkish":
                                    file_analysis = f"""üìä **OTOMATIK EDA ANALƒ∞Z SONU√áLARI**

üîç **VERƒ∞ SETƒ∞ √ñZETƒ∞:**
‚Ä¢ Boyut: {summary_data.get('dataset_shape', {}).get('rows', 'N/A')} satƒ±r √ó {summary_data.get('dataset_shape', {}).get('columns', 'N/A')} s√ºtun
‚Ä¢ Eksik deƒüerli s√ºtunlar: {len(summary_data.get('top_missing_columns', []))}
‚Ä¢ Olu≈üturulan grafik sayƒ±sƒ±: {summary_data.get('figures_generated', 0)}

üìà **√ñNE √áIKAN KORELASYONLAR:**
{chr(10).join([f"‚Ä¢ {corr['column1']} vs {corr['column2']}: {corr['correlation']:.3f}" for corr in summary_data.get('top_correlations', [])[:3]])}

üìÑ **DETAYLI RAPOR MEVCUT:** `reports/REPORT.md` dosyasƒ±nda tam analiz bulunuyor.

üí° **KULLANICININ SORULARINI CEVAPLAYABILIRSIN:** Bu analiz sonu√ßlarƒ±nƒ± kullanarak kullanƒ±cƒ±nƒ±n veri hakkƒ±ndaki sorularƒ±nƒ± cevaplayabilirsin."""
                                else:
                                    file_analysis = f"""üìä **AUTOMATED EDA ANALYSIS RESULTS**

üîç **DATASET SUMMARY:**
‚Ä¢ Size: {summary_data.get('dataset_shape', {}).get('rows', 'N/A')} rows √ó {summary_data.get('dataset_shape', {}).get('columns', 'N/A')} columns  
‚Ä¢ Columns with missing values: {len(summary_data.get('top_missing_columns', []))}
‚Ä¢ Generated visualizations: {summary_data.get('figures_generated', 0)}

üìà **TOP CORRELATIONS:**
{chr(10).join([f"‚Ä¢ {corr['column1']} vs {corr['column2']}: {corr['correlation']:.3f}" for corr in summary_data.get('top_correlations', [])[:3]])}

üìÑ **DETAILED REPORT AVAILABLE:** Full analysis is available in `reports/REPORT.md`.

üí° **YOU CAN ANSWER USER QUESTIONS:** Use these analysis results to answer the user's questions about their data."""
                            
                        self.logger.info(f"üìä Used EDA report as file analysis for file_id: {file_id}")
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è Could not read EDA report: {e}")
            
            # Select optimal model
            model = self._select_optimal_model(intent, file_id is not None)
            
            # Generate comprehensive system prompt with world-class knowledge
            system_prompt = self._get_comprehensive_system_prompt(language, intent, file_analysis)
            
            # Create enhanced message
            enhanced_message = self._create_enhanced_message(message, file_analysis, language, intent)
            
            # Generate AI response using OpenRouter
            if self.openrouter_client:
                # üî¨ EXPERT DATA SCIENTIST AI MODE
                # Check if we should use Expert AI instead of regular AI
                should_use_expert_ai = (
                    file_analysis or 
                    any(keyword in message.lower() for keyword in [
                        'analiz', 'analyze', 'analysis', 'model', 'veri', 'data', 
                        'grafik', 'chart', 'g√∂rselle≈ütir', 'visualize', 'insight',
                        '√∂neri', 'recommend', 'ml', 'machine learning', 'makine √∂ƒürenmesi',
                        'istatistik', 'statistic', 'korelasyon', 'correlation'
                    ])
                )
                
                if should_use_expert_ai:
                    self.logger.info("üî¨ Using Expert Data Scientist AI for response")
                    
                    # Try to get existing analysis results if file is available
                    expert_analysis_results = None
                    if file_id:
                        try:
                            expert_results = await self.analyze_file_with_expert_ai(file_id, user_id)
                            if expert_results.get("success", False):
                                expert_analysis_results = expert_results["expert_analysis"]
                        except Exception as e:
                            self.logger.warning(f"Could not get expert analysis: {e}")
                    
                    # Generate expert conversation response
                    expert_response = self.expert_conversation_ai.generate_expert_response(
                        message, expert_analysis_results
                    )
                    
                    # Add expert analysis details if available
                    if expert_analysis_results:
                        if language == "Turkish":
                            enhanced_expert_response = f"""üî¨ **EXPERT DATA SCIENTIST ANALYSIS**

{expert_response}

---

üìä **ANALIZ SONU√áLARI:**
‚Ä¢ Veri Kalitesi: {expert_analysis_results.get('data_quality', {}).get('quality_score', 'N/A')}/100
‚Ä¢ Dataset Boyutu: {expert_analysis_results.get('basic_info', {}).get('shape', ['N/A', 'N/A'])[0]} satƒ±r √ó {expert_analysis_results.get('basic_info', {}).get('shape', ['N/A', 'N/A'])[1]} s√ºtun
‚Ä¢ G√º√ßl√º Korelasyonlar: {len(expert_analysis_results.get('statistical_analysis', {}).get('correlations', {}).get('high_correlations', []))} adet
‚Ä¢ G√∂rselle≈ütirmeler: {len(expert_analysis_results.get('visualizations', {}))} adet olu≈üturuldu
‚Ä¢ ƒ∞≈ü √ñnerileri: {len(expert_analysis_results.get('business_insights', []))} actionable insight

üí° **Daha detaylƒ± analiz i√ßin '/api/v1/expert-analysis' endpoint'ini kullanabilirsiniz.**"""
                        else:
                            enhanced_expert_response = f"""üî¨ **EXPERT DATA SCIENTIST ANALYSIS**

{expert_response}

---

üìä **ANALYSIS RESULTS:**
‚Ä¢ Data Quality: {expert_analysis_results.get('data_quality', {}).get('quality_score', 'N/A')}/100
‚Ä¢ Dataset Size: {expert_analysis_results.get('basic_info', {}).get('shape', ['N/A', 'N/A'])[0]} rows √ó {expert_analysis_results.get('basic_info', {}).get('shape', ['N/A', 'N/A'])[1]} columns
‚Ä¢ Strong Correlations: {len(expert_analysis_results.get('statistical_analysis', {}).get('correlations', {}).get('high_correlations', []))} found
‚Ä¢ Visualizations: {len(expert_analysis_results.get('visualizations', {}))} created
‚Ä¢ Business Insights: {len(expert_analysis_results.get('business_insights', []))} actionable recommendations

üí° **For detailed analysis, use '/api/v1/expert-analysis' endpoint.**"""
                        
                        ai_response = enhanced_expert_response
                    else:
                        ai_response = f"üî¨ **EXPERT DATA SCIENTIST AI**\n\n{expert_response}"
                
                else:
                    # Regular AI response using OpenRouter
                    response = self.openrouter_client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": enhanced_message}
                        ],
                        max_tokens=2500,
                        temperature=0.7
                    )
                    
                    ai_response = response.choices[0].message.content
                
                # FORCE MODE: If AI ignores file context, override with direct analysis
                # üö® ULTIMATE FORCE MODE - Check for ANY file-not-found phrases
                force_triggers = [
                    "don't see", "no file", "not uploaded", "attached", "shared", 
                    "i apologize", "csv file", "can't access", "notice i can", 
                    "actually access", "file directly", "please upload", "could you please"
                ]
                should_force = file_analysis and any(trigger in ai_response.lower() for trigger in force_triggers)
                
                if should_force:
                    self.logger.warning("üö® AI ignored file context - forcing direct analysis")
                    if language == "Turkish":
                        ai_response = f"""üéØ Verilerinizi analiz ettim! ƒ∞≈üte bulgularƒ±m:

{file_analysis}

Bu analiz ger√ßek verilerinize dayanmaktadƒ±r. Daha detaylƒ± analiz istiyorsanƒ±z, spesifik sorular sorabilirsiniz!"""
                    else:
                        ai_response = f"""üéØ I've analyzed your data! Here are my findings:

{file_analysis}

This analysis is based on your actual data. Feel free to ask specific questions for deeper insights!"""
                
                # Add warmth and encouragement to the response
                ai_response = self.enhanced_conversation.add_warmth_to_response(ai_response, language)
                
                # Store in memory
                self.memory_system.maintain_context(user_id, {
                    'message': message,
                    'response': ai_response,
                    'intent': intent.value,
                    'language': language,
                    'file_id': file_id,
                    'timestamp': datetime.now().isoformat()
                })
                
                return ai_response
            
            else:
                return self._generate_fallback_response(message, language)
                
        except Exception as e:
            self.logger.error(f"‚ùå Error: {e}")
            if language == "Turkish":
                return "‚ö†Ô∏è Bir hata olu≈ütu, ancak size yardƒ±mcƒ± olmaya devam edeceƒüim. L√ºtfen tekrar deneyin."
            else:
                return "‚ö†Ô∏è An error occurred, but I'll continue helping you. Please try again."

    async def analyze_file_completely(self, file_id: str, user_id: str = "default_user") -> str:
        """Comprehensive file analysis for immediate analysis requests - supports OCR"""
        try:
            # Mark file as being analyzed
            self.file_manager.mark_file_analyzed(user_id, file_id)
            
            # Find the file path
            matching_files = [f for f in os.listdir(UPLOAD_DIR) if f.startswith(file_id)]
            if not matching_files:
                return "Sorry, I couldn't access the file for analysis. Please try uploading it again."
            
            file_path = os.path.join(UPLOAD_DIR, matching_files[0])
            
            # Use UniversalFileHandler for comprehensive analysis
            processing_result = self.universal_file_handler.detect_and_process(file_path)
            
            if not processing_result.get('processing_result', {}).get('success', False):
                return "Sorry, there was an error processing your file. Please try uploading it again."
            
            # Detect language preference (could be enhanced with user detection)
            language = "Turkish"  # Set to Turkish as user is Turkish
            
            # Get processing results
            proc_result = processing_result['processing_result']
            file_info = processing_result['file_info']
            
            # Check if this is OCR data (image with text extraction)
            is_ocr_data = 'ocr_analysis' in proc_result
            
            if is_ocr_data:
                # Get OCR analysis results
                ocr_analysis = proc_result['ocr_analysis']
                extracted_text = ocr_analysis.get('extracted_text', '')
                confidence = ocr_analysis.get('confidence_score', 0)
                
                # Check if we have structured data (table)
                structured_data = ocr_analysis.get('structured_data')
                
                if structured_data is not None:
                    # This is structured OCR data (table)
                    analysis_request = f"""Bu OCR ile √ßƒ±karƒ±lan tablo verisini analiz et:

Tablo Bilgileri:
- Boyut: {structured_data.shape[0]} satƒ±r √ó {structured_data.shape[1]} s√ºtun
- S√ºtunlar: {list(structured_data.columns)}
- OCR G√ºven Skoru: {confidence:.1f}%

Veri √ñrneƒüi:
{structured_data.head().to_string()}

L√ºtfen ≈üunlarƒ± analiz et:
1. Tablonun i√ßeriƒüi ve amacƒ±
2. Veri kalitesi ve eksik deƒüerler
3. √ñnemli trendler ve patterns
4. ƒ∞≈ü deƒüeri potansiyeli
5. √ñnerilen analizler
6. Python kod √∂rnekleri"""
                else:
                    # Text-only OCR data
                    analysis_request = f"""Bu OCR ile √ßƒ±karƒ±lan metni analiz et:

OCR Sonu√ßlarƒ±:
- G√ºven skoru: {confidence:.1f}%
- Metin uzunluƒüu: {len(extracted_text)} karakter
- Kelime sayƒ±sƒ±: {len(extracted_text.split())} kelime

√áƒ±karƒ±lan Metin:
{extracted_text}

L√ºtfen ≈üunlarƒ± analiz et:
1. Metnin i√ßeriƒüi ve t√ºr√º (fatura, tablo, form, vb.)
2. √ñnemli bilgiler ve sayƒ±sal veriler
3. Veri kalitesi deƒüerlendirmesi
4. Potansiyel kullanƒ±m alanlarƒ±
5. √ñneriler ve sonraki adƒ±mlar
6. Python ile nasƒ±l i≈ülenebileceƒüi"""
            else:
                # Regular file analysis based on format
                file_format = proc_result.get('format', 'unknown')
                
                if file_format in ['csv', 'excel', 'json', 'parquet']:
                    # Structured data analysis
                    if 'sample_data' in proc_result and proc_result['sample_data']:
                        sample_data = proc_result['sample_data'][:5]  # First 5 rows
                        columns = proc_result.get('columns', [])
                        data_shape = proc_result.get('data_shape', [0, 0])
                        
                        analysis_request = f"""Bu veri setini kapsamlƒ± olarak analiz et:

Veri Seti Genel Bilgileri:
- Format: {file_format.upper()}
- Boyut: {data_shape[0]:,} satƒ±r √ó {data_shape[1]} s√ºtun
- S√ºtunlar: {columns}

√ñrnek Veriler (ilk 5 satƒ±r):
{sample_data}

L√ºtfen ≈üunlarƒ± analiz et:
1. Veri kalitesi deƒüerlendirmesi
2. Ana bulgular ve trendler
3. Analiz √∂nerileri
4. Sonraki adƒ±mlar
5. ƒ∞≈ü deƒüeri potansiyeli
6. Python kod √∂rnekleri"""
                
                elif file_format == 'pdf':
                    # PDF document analysis
                    text_content = proc_result.get('text_content', '')
                    page_count = proc_result.get('page_count', 0)
                    tables_found = proc_result.get('tables_found', 0)
                    
                    analysis_request = f"""Bu PDF belgesini analiz et:

PDF Bilgileri:
- Sayfa sayƒ±sƒ±: {page_count}
- Bulunan tablo sayƒ±sƒ±: {tables_found}
- Metin √ßƒ±karma durumu: {'‚úÖ Ba≈üarƒ±lƒ±' if proc_result.get('text_extracted') else '‚ùå Ba≈üarƒ±sƒ±z'}

ƒ∞√ßerik √ñrneƒüi:
{text_content[:500]}...

L√ºtfen ≈üunlarƒ± analiz et:
1. Belgenin t√ºr√º ve amacƒ±
2. √ñnemli bilgi ve veriler
3. Yapƒ±sal analiz
4. Potansiyel kullanƒ±m alanlarƒ±
5. √ñneriler ve sonraki adƒ±mlar"""
                
                elif file_format == 'docx':
                    # Word document analysis
                    sample_text = proc_result.get('sample_text', '')
                    word_count = proc_result.get('word_count', 0)
                    table_count = proc_result.get('table_count', 0)
                    
                    analysis_request = f"""Bu Word belgesini analiz et:

Belge Bilgileri:
- Kelime sayƒ±sƒ±: {word_count}
- Tablo sayƒ±sƒ±: {table_count}
- Paragraf sayƒ±sƒ±: {proc_result.get('paragraph_count', 0)}

ƒ∞√ßerik √ñrneƒüi:
{sample_text}

L√ºtfen ≈üunlarƒ± analiz et:
1. Belgenin t√ºr√º ve amacƒ±
2. Ana i√ßerik ve yapƒ±
3. √ñnemli bilgiler
4. Potansiyel kullanƒ±m alanlarƒ±
5. √ñneriler ve sonraki adƒ±mlar"""
                
                else:
                    # Generic file analysis
                    analysis_request = f"""Bu dosyayƒ± analiz et:

Dosya Bilgileri:
- Format: {file_format.upper()}
- Boyut: {file_info.get('size_mb', 0):.2f} MB
- ƒ∞≈üleme durumu: {'‚úÖ Ba≈üarƒ±lƒ±' if proc_result.get('success') else '‚ùå Ba≈üarƒ±sƒ±z'}

ƒ∞≈üleme Sonu√ßlarƒ±:
{str(proc_result)[:1000]}

L√ºtfen bu dosya hakkƒ±nda genel bir analiz ve deƒüerlendirme yap."""
            
            # Generate comprehensive system prompt
            system_prompt = self._get_comprehensive_system_prompt(language, IntentCategory.DATA_ANALYSIS, "")
            
            if self.openrouter_client:
                response = self.openrouter_client.chat.completions.create(
                    model=self.model_config["technical"],
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": analysis_request}
                    ],
                    max_tokens=3000,
                    temperature=0.7
                )
            
                ai_response = response.choices[0].message.content
                
                # Combine with OCR technical analysis if available
                if is_ocr_data and ocr_analysis:
                    combined_response = f"""{ai_response}

---

üìä **TEKNƒ∞K OCR ANALƒ∞Z DETAYLARI:**

{ocr_analysis}"""
                    return self.enhanced_conversation.add_warmth_to_response(combined_response, language)
                else:
                    return self.enhanced_conversation.add_warmth_to_response(ai_response, language)
            else:
                return self._generate_fallback_analysis_response(file_data, language)
                
        except Exception as e:
            self.logger.error(f"‚ùå File analysis error: {e}")
            return "I encountered an error while analyzing your file. Please try again or upload a different file."

    def _get_comprehensive_system_prompt(self, language: str, intent: IntentCategory, file_analysis: str) -> str:
        """Enhanced system prompt with world-class data science knowledge"""
        
        if language == "Turkish":
            base_prompt = """Sen DataSoph AI'sƒ±n - d√ºnyanƒ±n en geli≈ümi≈ü, en bilgili ve en kibar veri bilimi uzmanƒ±sƒ±n.

üß† SENƒ∞N KAPSAMLI Bƒ∞LGƒ∞N:

üìä VERƒ∞ Bƒ∞Lƒ∞Mƒ∞ UZMANI:
- ƒ∞statistik: Bayesian inference, frequentist methods, hypothesis testing, A/B testing, causal inference, survival analysis
- Machine Learning: Supervised/unsupervised learning, deep learning, reinforcement learning, AutoML, few-shot learning
- Algoritmalar: Classical ML (SVM, Random Forest, XGBoost, LightGBM, CatBoost), neural networks (CNN, RNN, LSTM, Transformers, GANs, VAEs)
- Feature Engineering: Advanced selection techniques, automated feature creation, interaction terms, polynomial features
- Model Evaluation: Advanced cross-validation, Bayesian optimization, model interpretation (SHAP, LIME, permutation importance, anchor explanations)

üõ†Ô∏è TEKNƒ∞K ARA√áLAR USTASI:
- Programming: Python (pandas, numpy, scikit-learn, tensorflow, pytorch, jax), R, SQL, Scala, Julia
- Visualization: Matplotlib, Seaborn, Plotly, D3.js, Bokeh, Altair, Tableau, PowerBI
- Big Data: Hadoop, Spark, Kafka, Flink, distributed computing, data lakes
- Cloud Platforms: AWS (SageMaker, S3, EMR), GCP (BigQuery, Vertex AI), Azure ML, Databricks
- MLOps: Docker, Kubernetes, Kubeflow, MLflow, DVC, CI/CD pipelines, model monitoring

üíº ƒ∞≈û ANLAYI≈ûI:
- Industry Applications: Healthcare, finance, retail, manufacturing, tech, energy, telecommunications
- Business Metrics: ROI, customer lifetime value, churn prediction, price optimization, demand forecasting
- Strategic Planning: Data strategy, team building, technology roadmaps, budget planning
- Ethics & Governance: Bias detection, fairness metrics, privacy-preserving ML, GDPR compliance

üéØ MODERN TRENDS:
- Generative AI: Large language models, diffusion models, prompt engineering, fine-tuning
- MLOps: Model versioning, automated retraining, drift detection, A/B testing for ML
- Explainable AI: Interpretable models, counterfactual explanations, regulatory compliance
- Edge Computing: Mobile ML, federated learning, on-device inference
- Advanced Analytics: Graph neural networks, time series forecasting, causal ML, meta-learning

üí° Kƒ∞≈ûƒ∞Lƒ∞K VE YAKLA≈ûIM:
- Son derece kibar, sabƒ±rlƒ± ve destekleyici
- Karma≈üƒ±k konularƒ± basit terimlerle a√ßƒ±klayabilen
- Practical √∂rnekler veren, actionable insights sunan
- Hem technical hem business perspektifi birle≈ütiren
- S√ºrekli √∂ƒürenmeyi te≈üvik eden, meraklƒ± ve ilham verici
- Hatalardan √∂ƒürenmeyi normal kar≈üƒ±layan, yapƒ±cƒ±
- Her seviyedeki kullanƒ±cƒ±ya uyum saƒülayabilen (beginner to expert)

üó£Ô∏è KONU≈ûMA STƒ∞Lƒ∞N:
- Sƒ±cak ve samimi ama profesyonel
- A√ßƒ±k ve anla≈üƒ±lƒ±r a√ßƒ±klamalar
- √ñrneklerle desteklenen teorik bilgi
- Adƒ±m adƒ±m rehberlik
- Cesaretlendirici ve motivasyonlu
- Soru sormaya te≈üvik edici
- "Birlikte ke≈üfedelim" yakla≈üƒ±mƒ±

üì∑ OCR & G√ñRSEL ANALƒ∞Z USTASI:
- Taranmƒ±≈ü belgeler, fotoƒüraflar ve ekran g√∂r√ºnt√ºlerinden metin √ßƒ±karma
- Resimlerdeki tablolarƒ± pandas DataFrame'e √ßevirme  
- El yazƒ±sƒ± metinleri dijitalle≈ütirme
- Fatura, makbuz, finansal belge i≈üleme
- Kaƒüƒ±t anket formlarƒ±nƒ± dataset'e √ßevirme
- Dashboard screenshot'larƒ±ndan KPI √ßƒ±karma
- √áoklu dil desteƒüi (T√ºrk√ße, ƒ∞ngilizce, 90+ dil)
- Metin yapƒ±sƒ±nƒ± koruyarak akƒ±llƒ± formatlama

üî¨ EXPERT DATA SCIENTIST YETENEKLER:
- Comprehensive EDA: Otomatik veri ke≈üfi, kalite deƒüerlendirmesi, istatistiksel analiz
- Advanced Visualizations: Publication-quality charts, correlation heatmaps, distribution analysis
- Predictive Modeling: Automated model selection, hyperparameter tuning, performance evaluation
- Business Intelligence: ROI analysis, actionable insights, strategic recommendations
- Statistical Testing: Hypothesis testing, significance analysis, confidence intervals
- Feature Engineering: Automated feature creation, selection, and transformation

üî• √ñNEMLƒ∞: Her zaman executable Python kodu yaz ve sonu√ßlarƒ± yorumla!

Her soruya ≈üu yakla≈üƒ±mla yanƒ±t ver:
1. Sorunu anlayƒ±p empati kurun
2. A√ßƒ±k ve dostane bir a√ßƒ±klama yap
3. Pratik kod √∂rnekleri ver
4. Sonu√ßlarƒ± business value ile baƒüla
5. Sonraki adƒ±mlarƒ± √∂ner
6. √ñƒürenmeye te≈üvik et

Sen sadece bir AI deƒüil, g√ºvenilir bir veri bilimi ment√∂r√ºs√ºn! ‚ù§Ô∏è"""
        
        else:  # English
            base_prompt = """You are DataSoph AI - the world's most advanced, knowledgeable, and genuinely caring data science expert.

üß† YOUR COMPREHENSIVE EXPERTISE:

üìä DATA SCIENCE MASTERY:
- Statistics: Bayesian inference, frequentist methods, hypothesis testing, A/B testing, causal inference, survival analysis
- Machine Learning: Supervised/unsupervised learning, deep learning, reinforcement learning, AutoML, few-shot learning
- Algorithms: Classical ML (SVM, Random Forest, XGBoost, LightGBM, CatBoost), neural networks (CNN, RNN, LSTM, Transformers, GANs, VAEs)
- Feature Engineering: Advanced selection techniques, automated feature creation, interaction terms, polynomial features
- Model Evaluation: Advanced cross-validation, Bayesian optimization, model interpretation (SHAP, LIME, permutation importance, anchor explanations)

üõ†Ô∏è TECHNICAL MASTERY:
- Programming: Python (pandas, numpy, scikit-learn, tensorflow, pytorch, jax), R, SQL, Scala, Julia
- Visualization: Matplotlib, Seaborn, Plotly, D3.js, Bokeh, Altair, Tableau, PowerBI
- Big Data: Hadoop, Spark, Kafka, Flink, distributed computing, data lakes
- Cloud Platforms: AWS (SageMaker, S3, EMR), GCP (BigQuery, Vertex AI), Azure ML, Databricks
- MLOps: Docker, Kubernetes, Kubeflow, MLflow, DVC, CI/CD pipelines, model monitoring

üíº BUSINESS INTELLIGENCE:
- Industry Applications: Healthcare, finance, retail, manufacturing, tech, energy, telecommunications
- Business Metrics: ROI, customer lifetime value, churn prediction, price optimization, demand forecasting
- Strategic Planning: Data strategy, team building, technology roadmaps, budget planning
- Ethics & Governance: Bias detection, fairness metrics, privacy-preserving ML, GDPR compliance

üéØ CUTTING-EDGE KNOWLEDGE:
- Generative AI: Large language models, diffusion models, prompt engineering, fine-tuning
- MLOps: Model versioning, automated retraining, drift detection, A/B testing for ML
- Explainable AI: Interpretable models, counterfactual explanations, regulatory compliance
- Edge Computing: Mobile ML, federated learning, on-device inference
- Advanced Analytics: Graph neural networks, time series forecasting, causal ML, meta-learning

üí° PERSONALITY & APPROACH:
- Exceptionally kind, patient, and supportive
- Able to explain complex concepts in simple, relatable terms
- Provides practical examples and actionable insights
- Combines technical depth with business perspective
- Encourages continuous learning and curiosity
- Treats mistakes as learning opportunities
- Adapts to all skill levels (beginner to expert)

üó£Ô∏è COMMUNICATION STYLE:
- Warm and friendly yet professional
- Clear and engaging explanations
- Theory supported by practical examples
- Step-by-step guidance when needed
- Encouraging and motivational
- Promotes asking questions
- "Let's explore this together" approach

üîç OCR & IMAGE ANALYSIS EXPERT:
- Optical Character Recognition for scanned documents, photos, screenshots
- Table extraction from images ‚Üí Convert to pandas DataFrame
- Handwritten text recognition and digitization
- Financial document processing (invoices, receipts, statements)
- Survey form digitization ‚Üí Dataset creation
- Dashboard screenshot analysis ‚Üí Extract KPIs and metrics
- Multi-language text extraction (English, Turkish, 90+ languages)
- Text structure preservation and intelligent formatting

üî¨ EXPERT DATA SCIENTIST CAPABILITIES:
- Comprehensive EDA: Automated data discovery, quality assessment, statistical analysis
- Advanced Visualizations: Publication-quality charts, correlation heatmaps, distribution analysis
- Predictive Modeling: Automated model selection, hyperparameter tuning, performance evaluation
- Business Intelligence: ROI analysis, actionable insights, strategic recommendations
- Statistical Testing: Hypothesis testing, significance analysis, confidence intervals
- Feature Engineering: Automated feature creation, selection, and transformation

üî• CRITICAL: Always write executable Python code and interpret results!

Approach every question with:
1. Understanding and empathy for the user's situation
2. Clear, friendly explanations
3. Practical code examples that work
4. Connection to business value
5. Suggestions for next steps
6. Encouragement for continued learning

You're not just an AI - you're a trusted data science mentor who genuinely cares about helping people succeed! ‚ù§Ô∏è"""

        # Add file analysis if available
        if file_analysis:
            if language == "Turkish":
                base_prompt += f"\n\nüìä **MEVCUT VERƒ∞ SETƒ∞ ANALƒ∞Zƒ∞:**\n{file_analysis}\n\nüö® KRƒ∞Tƒ∞K UYARI: KULLANICI ZATEN DOSYA Y√úKLEDƒ∞! Yukarƒ±daki analiz ger√ßek veridir. 'Dosya yok' deme, yukarƒ±daki veri analizini kullan!"
            else:
                base_prompt += f"\n\nüìä **CURRENT DATASET ANALYSIS:**\n{file_analysis}\n\nüö® CRITICAL: THE USER HAS ALREADY UPLOADED A FILE! The analysis above is real data. DO NOT say 'no file uploaded', use the data analysis above!"

        return base_prompt

    def _generate_enhanced_greeting(self, language: str) -> str:
        """Generate enhanced greeting response"""
        if language == "Turkish":
            return """üåü **Merhaba! DataSoph AI'ya ho≈ü geldiniz!**

Ben d√ºnyanƒ±n en geli≈ümi≈ü veri bilimi asistanƒ±yƒ±m ve sizinle √ßalƒ±≈ümaktan √ßok heyecanlƒ±yƒ±m! üöÄ

üéØ **Yeteneklerim:**
‚Ä¢ üìä Kapsamlƒ± veri analizi ve EDA
‚Ä¢ ü§ñ Makine √∂ƒürenmesi model geli≈ütirme  
‚Ä¢ üìà ƒ∞leri seviye g√∂rselle≈ütirmeler
‚Ä¢ üíº ƒ∞≈ü zekasƒ± ve ROI analizi
‚Ä¢ üîç ƒ∞statistiksel analiz ve hipotez testleri
‚Ä¢ üíª Otomatik kod √ºretimi ve √ßalƒ±≈ütƒ±rma
‚Ä¢ üé® Geli≈ümi≈ü model interpretability
‚Ä¢ üåê Modern MLOps ve deployment stratejileri
‚Ä¢ üì∑ OCR ile g√∂rsel analiz (fotoƒüraf, taranmƒ±≈ü belge, tablo √ßƒ±karma)

üí° **Nasƒ±l yardƒ±mcƒ± olabilirim?**
- Veri dosyanƒ±zƒ± y√ºkleyin (CSV, Excel, JSON destekleniyor)
- Veri bilimi sorunuzu sorun
- ƒ∞statistiksel analiz konularƒ±nda rehberlik isteyin
- ML model √∂nerileri i√ßin danƒ±≈üƒ±n

Birlikte harika ≈üeyler ba≈üaracaƒüƒ±z! ‚ú®"""
        else:
            return """üåü **Hello! Welcome to DataSoph AI!**

I'm the world's most advanced data science assistant, and I'm absolutely thrilled to work with you! üöÄ

üéØ **My Capabilities:**
‚Ä¢ üìä Comprehensive data analysis and EDA
‚Ä¢ ü§ñ Machine learning model development
‚Ä¢ üìà Advanced visualizations
‚Ä¢ üíº Business intelligence and ROI analysis  
‚Ä¢ üîç Statistical analysis and hypothesis testing
‚Ä¢ üíª Automatic code generation and execution
‚Ä¢ üé® Advanced model interpretability
‚Ä¢ üåê Modern MLOps and deployment strategies
‚Ä¢ üì∑ OCR & Visual Analysis (photos, scanned docs, table extraction)

üí° **How can I help you today?**
- Upload your data file (CSV, Excel, JSON supported)
- Ask your data science questions
- Get guidance on statistical analysis
- Consult on ML model recommendations

Let's create something amazing together! ‚ú®"""

    def _select_optimal_model(self, intent: IntentCategory, has_file: bool) -> str:
        """Select optimal model based on context"""
        if has_file or intent == IntentCategory.DATA_ANALYSIS:
            return self.model_config["technical"]
        elif intent in [IntentCategory.GREETING, IntentCategory.CASUAL_CHAT]:
            return self.model_config["casual"]
        else:
            return self.model_config["analysis"]

    def _create_enhanced_message(self, message: str, file_analysis: str, language: str, intent: IntentCategory) -> str:
        """Create enhanced message with context"""
        
        enhanced_parts = [f"KULLANICI MESAJI: {message}" if language == "Turkish" else f"USER MESSAGE: {message}"]
        
        if file_analysis:
            if language == "Turkish":
                enhanced_parts.append("üö® DOSYA MEVCUT: Kullanƒ±cƒ± dosya y√ºkledi ve a≈üaƒüƒ±da analizi var:")
            else:
                enhanced_parts.append("üö® FILE AVAILABLE: User has uploaded a file and analysis is below:")
            enhanced_parts.append(file_analysis)
            if language == "Turkish":
                enhanced_parts.append("‚ö†Ô∏è ASLA 'dosya yok' deme! Yukarƒ±daki veri analizini kullan!")
            else:
                enhanced_parts.append("‚ö†Ô∏è NEVER say 'no file uploaded'! Use the data analysis above!")
        
        if language == "Turkish":
            enhanced_parts.append("BEKLENEN YAKLA≈ûIM: Uzman seviyesinde analiz, pratik √∂neriler, kod √∂rnekleri ve i≈ü deƒüeri perspektifi")
        else:
            enhanced_parts.append("EXPECTED APPROACH: Expert-level analysis, practical recommendations, code examples and business value perspective")
        
        return "\n\n".join(enhanced_parts)

    async def _analyze_file_advanced(self, file_id: str, language: str) -> str:
        """Enhanced file analysis with actual content - supports OCR results"""
        try:
            self.logger.info(f"üî¨ Starting enhanced file analysis for: {file_id}")
            file_data = self._get_file_data(file_id)
            if file_data is None:
                self.logger.warning(f"‚ùå No file data found for: {file_id}")
                return ""
            
            self.logger.info(f"üìä File data loaded successfully: {file_data.shape}")
            
            # Check if this is OCR extracted data
            is_ocr_data = 'extracted_text' in file_data.columns or 'source_file' in file_data.columns
            
            if is_ocr_data:
                # Special handling for OCR results
                if language == "Turkish":
                    if 'extracted_text' in file_data.columns:
                        extracted_text = file_data.iloc[0]['extracted_text']
                        confidence = file_data.iloc[0].get('confidence_score', 0)
                        source_file = file_data.iloc[0].get('source_file', 'Unknown')
                        
                        analysis = f"""üì∑ OCR G√ñRSEL ANALƒ∞Z SONU√áLARI:

üîç DOSYA Bƒ∞LGƒ∞LERƒ∞:
‚Ä¢ Kaynak dosya: {source_file}
‚Ä¢ OCR g√ºven skoru: {confidence:.2f} ({self._get_confidence_level(confidence)})
‚Ä¢ √áƒ±karƒ±lan metin uzunluƒüu: {len(extracted_text)} karakter
‚Ä¢ ƒ∞≈üleme y√∂ntemi: OCR (Optical Character Recognition)

üìù √áIKARILAN METƒ∞N:
{extracted_text}

üîç METƒ∞N ANALƒ∞Zƒ∞:
‚Ä¢ Kelime sayƒ±sƒ±: {len(extracted_text.split())} kelime
‚Ä¢ Satƒ±r sayƒ±sƒ±: {len(extracted_text.split('\n'))} satƒ±r
‚Ä¢ Sayƒ± i√ßeriyor mu: {'Evet' if any(char.isdigit() for char in extracted_text) else 'Hayƒ±r'}
‚Ä¢ Finansal veri: {'Evet' if any(symbol in extracted_text for symbol in ['‚Ç∫', '$', '‚Ç¨', '¬£']) else 'Hayƒ±r'}

üí° ANLAM VE √ñNERƒ∞LER:
Bu g√∂rselden ba≈üarƒ±yla metin √ßƒ±kardƒ±m! ≈ûimdi bu veriyi analiz edebilir, temizleyebilir veya ba≈üka formatlara d√∂n√º≈üt√ºrebilirim."""
                    else:
                        # This is structured OCR data (table)
                        analysis = f"""üìä OCR TABLO ANALƒ∞Z SONU√áLARI:

üîç GENEL Bƒ∞LGƒ∞LER:
‚Ä¢ Boyut: {file_data.shape[0]:,} satƒ±r √ó {file_data.shape[1]} s√ºtun
‚Ä¢ Eksik deƒüer: {file_data.isnull().sum().sum()} adet
‚Ä¢ Sayƒ±sal s√ºtun: {len(file_data.select_dtypes(include=[np.number]).columns)}
‚Ä¢ Kategorik s√ºtun: {len(file_data.select_dtypes(include=['object']).columns)}

üìã S√úTUN ƒ∞Sƒ∞MLERƒ∞:
{', '.join(file_data.columns.tolist())}

üìà ƒ∞LK 5 SATIR:
{file_data.head().to_string()}

üìä SAYISAL ƒ∞STATƒ∞STƒ∞KLER:
{file_data.describe().to_string()}

üí° OCR TABLO BA≈ûARIYLA √áIKARILDI:
Bu g√∂rseldeeki tablo ba≈üarƒ±yla pandas DataFrame'e d√∂n√º≈üt√ºr√ºld√º! Artƒ±k bu veriyi analiz edebilir, g√∂rselle≈ütirebilir ve manip√ºle edebilirim."""
                        
                else:  # English
                    if 'extracted_text' in file_data.columns:
                        extracted_text = file_data.iloc[0]['extracted_text']
                        confidence = file_data.iloc[0].get('confidence_score', 0)
                        source_file = file_data.iloc[0].get('source_file', 'Unknown')
                        
                        analysis = f"""üì∑ OCR IMAGE ANALYSIS RESULTS:

üîç FILE INFORMATION:
‚Ä¢ Source file: {source_file}
‚Ä¢ OCR confidence score: {confidence:.2f} ({self._get_confidence_level(confidence)})
‚Ä¢ Extracted text length: {len(extracted_text)} characters
‚Ä¢ Processing method: OCR (Optical Character Recognition)

üìù EXTRACTED TEXT:
{extracted_text}

üîç TEXT ANALYSIS:
‚Ä¢ Word count: {len(extracted_text.split())} words
‚Ä¢ Line count: {len(extracted_text.split('\n'))} lines
‚Ä¢ Contains numbers: {'Yes' if any(char.isdigit() for char in extracted_text) else 'No'}
‚Ä¢ Financial data: {'Yes' if any(symbol in extracted_text for symbol in ['‚Ç∫', '$', '‚Ç¨', '¬£']) else 'No'}

üí° INSIGHTS AND RECOMMENDATIONS:
Successfully extracted text from your image! I can now analyze, clean, or transform this data into different formats."""
                    else:
                        # This is structured OCR data (table)
                        analysis = f"""üìä OCR TABLE ANALYSIS RESULTS:

üîç OVERVIEW:
‚Ä¢ Shape: {file_data.shape[0]:,} rows √ó {file_data.shape[1]} columns  
‚Ä¢ Missing values: {file_data.isnull().sum().sum()} total
‚Ä¢ Numerical columns: {len(file_data.select_dtypes(include=[np.number]).columns)}
‚Ä¢ Categorical columns: {len(file_data.select_dtypes(include=['object']).columns)}

üìã COLUMN NAMES:
{', '.join(file_data.columns.tolist())}

üìà FIRST 5 ROWS:
{file_data.head().to_string()}

üìä NUMERICAL STATISTICS:
{file_data.describe().to_string()}

üí° OCR TABLE SUCCESSFULLY EXTRACTED:
Successfully converted the table from your image into a pandas DataFrame! I can now analyze, visualize, and manipulate this data."""
            
            else:
                # Regular structured data analysis
                if language == "Turkish":
                    analysis = f"""üìä VERƒ∞ SETƒ∞ DETAYLI ANALƒ∞Zƒ∞:

üîç GENEL Bƒ∞LGƒ∞LER:
‚Ä¢ Boyut: {file_data.shape[0]:,} satƒ±r √ó {file_data.shape[1]} s√ºtun
‚Ä¢ Eksik deƒüer: {file_data.isnull().sum().sum()} adet
‚Ä¢ Sayƒ±sal s√ºtun: {len(file_data.select_dtypes(include=[np.number]).columns)}
‚Ä¢ Kategorik s√ºtun: {len(file_data.select_dtypes(include=['object']).columns)}

üìã S√úTUN ƒ∞Sƒ∞MLERƒ∞:
{', '.join(file_data.columns.tolist())}

üìà ƒ∞LK 5 SATIR:
{file_data.head().to_string()}

üìä SAYISAL ƒ∞STATƒ∞STƒ∞KLER:
{file_data.describe().to_string()}

üè∑Ô∏è VERƒ∞ Tƒ∞PLERƒ∞:
{file_data.dtypes.to_string()}"""

                # Eksik deƒüerler varsa detayƒ±nƒ± ekle
                if file_data.isnull().sum().sum() > 0:
                    analysis += f"\n\n‚ùå EKSƒ∞K DEƒûER DETAYI:\n{file_data.isnull().sum().to_string()}"
                
                # Kategorik s√ºtunlar varsa daƒüƒ±lƒ±mlarƒ±nƒ± ekle
                categorical_cols = file_data.select_dtypes(include=['object']).columns
                if len(categorical_cols) > 0:
                    analysis += "\n\nüè∑Ô∏è KATEGORƒ∞K S√úTUN DAƒûILIMLARI:"
                    for col in categorical_cols:
                        analysis += f"\n\n{col}:\n{file_data[col].value_counts().to_string()}"
                        
                else:  # English
                    analysis = f"""üìä COMPREHENSIVE DATASET ANALYSIS:

üîç OVERVIEW:
‚Ä¢ Shape: {file_data.shape[0]:,} rows √ó {file_data.shape[1]} columns  
‚Ä¢ Missing values: {file_data.isnull().sum().sum()} total
‚Ä¢ Numerical columns: {len(file_data.select_dtypes(include=[np.number]).columns)}
‚Ä¢ Categorical columns: {len(file_data.select_dtypes(include=['object']).columns)}

üìã COLUMN NAMES:
{', '.join(file_data.columns.tolist())}

üìà FIRST 5 ROWS:
{file_data.head().to_string()}

üìä NUMERICAL STATISTICS:
{file_data.describe().to_string()}

üè∑Ô∏è DATA TYPES:
{file_data.dtypes.to_string()}"""

                    # Add missing values detail if any
                    if file_data.isnull().sum().sum() > 0:
                        analysis += f"\n\n‚ùå MISSING VALUES DETAIL:\n{file_data.isnull().sum().to_string()}"
                    
                    # Add categorical distributions if any
                    categorical_cols = file_data.select_dtypes(include=['object']).columns
                    if len(categorical_cols) > 0:
                        analysis += "\n\nüè∑Ô∏è CATEGORICAL DISTRIBUTIONS:"
                        for col in categorical_cols:
                            analysis += f"\n\n{col}:\n{file_data[col].value_counts().to_string()}"
            
            self.logger.info(f"‚úÖ Enhanced file analysis completed: {len(analysis)} characters")
            return analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå File analysis error: {e}")
            return ""

    def _get_confidence_level(self, confidence: float) -> str:
        """Get confidence level description"""
        if confidence >= 0.8:
            return "Y√ºksek / High"
        elif confidence >= 0.6:
            return "Orta / Medium"
        elif confidence >= 0.4:
            return "D√º≈ü√ºk / Low"
        else:
            return "√áok D√º≈ü√ºk / Very Low"

    async def _execute_and_enhance_response(self, response: str, file_id: str, language: str) -> str:
        """Execute code and enhance response"""
        
        import re
        code_blocks = re.findall(r'```python\n(.*?)```', response, re.DOTALL)
        
        if not code_blocks:
            return response
        
        # Prepare execution context
        data_context = {}
        if file_id:
            file_data = self._get_file_data(file_id)
            if file_data is not None:
                data_context = {'df': file_data, 'data': file_data}
        
        # Execute code blocks
        execution_results = []
        for code in code_blocks:
            result = self.secure_executor.execute_code_safely(code.strip(), data_context)
            execution_results.append(result)
        
        # Add results to response
        if execution_results:
            if language == "Turkish":
                response += "\n\nüî• **KOD √áALI≈ûTIRMA SONU√áLARI:**\n"
            else:
                response += "\n\nüî• **CODE EXECUTION RESULTS:**\n"
            
            for i, result in enumerate(execution_results, 1):
                if result['success']:
                    if language == "Turkish":
                        response += f"\n**‚úÖ Kod Bloƒüu {i} - Ba≈üarƒ±yla √áalƒ±≈ütƒ±rƒ±ldƒ±**\n"
                    else:
                        response += f"\n**‚úÖ Code Block {i} - Successfully Executed**\n"
                    
                    if result['output']:
                        response += f"```\n{result['output']}\n```\n"
                else:
                    if language == "Turkish":
                        response += f"\n**‚ùå Kod Bloƒüu {i} - Hata:** {result['error']}\n"
                    else:
                        response += f"\n**‚ùå Code Block {i} - Error:** {result['error']}\n"
        
        return response

    def _generate_fallback_response(self, message: str, language: str) -> str:
        """Generate fallback response when AI is not available"""
        if language == "Turkish":
            return """‚ö†Ô∏è AI servisim ≈üu anda aktif deƒüil, ancak DataSoph AI yetenekleri:

üìä **Veri Analizi:**
‚Ä¢ Otomatik EDA ve veri kalitesi deƒüerlendirmesi
‚Ä¢ ƒ∞statistiksel analiz ve hipotez testleri
‚Ä¢ Geli≈ümi≈ü g√∂rselle≈ütirmeler

ü§ñ **Makine √ñƒürenmesi:**
‚Ä¢ Otomatik model se√ßimi ve hyperparameter tuning
‚Ä¢ Cross-validation ve performance evaluation
‚Ä¢ Feature engineering √∂nerileri

üíº **ƒ∞≈ü Zekasƒ±:**
‚Ä¢ ROI hesaplamalarƒ± ve i≈ü etkisi analizi
‚Ä¢ Executive summary ve actionable insights

L√ºtfen sorunuzu tekrar sorun veya bir veri dosyasƒ± y√ºkleyin."""
        else:
            return """‚ö†Ô∏è AI service not currently active, but DataSoph AI capabilities include:

üìä **Data Analysis:**
‚Ä¢ Automated EDA and data quality assessment
‚Ä¢ Statistical analysis and hypothesis testing
‚Ä¢ Advanced visualizations

ü§ñ **Machine Learning:**
‚Ä¢ Automatic model selection and hyperparameter tuning
‚Ä¢ Cross-validation and performance evaluation
‚Ä¢ Feature engineering recommendations

üíº **Business Intelligence:**
‚Ä¢ ROI calculations and business impact analysis
‚Ä¢ Executive summaries and actionable insights

Please restate your question or upload a data file."""

    def _detect_language(self, text: str) -> str:
        """Detect language"""
        turkish_words = ['merhaba', 'nasƒ±lsƒ±n', 'naber', 'selam', 'veri', 'analiz', 'nedir', 'nasƒ±l']
        english_words = ['hello', 'hi', 'how', 'data', 'analysis', 'what', 'can', 'help']
        
        text_lower = text.lower()
        turkish_count = sum(1 for word in turkish_words if word in text_lower)
        english_count = sum(1 for word in english_words if word in text_lower)
        
        return "Turkish" if turkish_count >= english_count else "English"

    def _get_file_data(self, file_id: str) -> Optional[pd.DataFrame]:
        """Get file data - now supports OCR results from images"""
        try:
            self.logger.info(f"üîç Looking for file with ID: {file_id}")
            print(f"DEBUG: Looking for file with ID: {file_id}")
            matching_files = [f for f in os.listdir(UPLOAD_DIR) if f.startswith(file_id)]
            self.logger.info(f"üìÅ Found matching files: {matching_files}")
            print(f"DEBUG: Found matching files: {matching_files}")
            
            if not matching_files:
                self.logger.warning(f"‚ùå No files found for ID: {file_id}")
                print(f"DEBUG: No files found for ID: {file_id}")
                return None
            
            file_path = os.path.join(UPLOAD_DIR, matching_files[0])
            filename = matching_files[0]
            self.logger.info(f"üìÑ Reading file: {filename} from {file_path}")
            
            # Handle structured data files
            if filename.lower().endswith('.csv'):
                data = pd.read_csv(file_path)
                self.logger.info(f"üìä CSV data shape: {data.shape}")
                self.logger.info(f"üìã CSV columns: {list(data.columns)}")
                return data
            elif filename.lower().endswith(('.xlsx', '.xls')):
                data = pd.read_excel(file_path)
                self.logger.info(f"üìä Excel data shape: {data.shape}")
                return data
            elif filename.lower().endswith('.json'):
                data = pd.read_json(file_path)
                self.logger.info(f"üìä JSON data shape: {data.shape}")
                return data
            
            # Handle image files with OCR
            elif filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.webp', '.heic')):
                self.logger.info(f"üîç Processing image file with OCR: {filename}")
                
                try:
                    # Use UniversalFileHandler to get OCR results
                    processing_result = self.universal_file_handler.detect_and_process(file_path)
                    ocr_analysis = processing_result.get('processing_result', {}).get('ocr_analysis', {})
                    
                    if ocr_analysis:
                        # Check if we have structured data (table) from OCR
                        structured_data = ocr_analysis.get('structured_data')
                        if structured_data is not None and isinstance(structured_data, pd.DataFrame):
                            self.logger.info(f"üìä OCR extracted table with shape: {structured_data.shape}")
                            return structured_data
                        
                        # If no table, create a simple DataFrame with extracted text
                        extracted_text = ocr_analysis.get('extracted_text', '')
                        if extracted_text:
                            self.logger.info(f"üìù OCR extracted {len(extracted_text)} characters of text")
                            # Create a simple DataFrame with the extracted text
                            text_df = pd.DataFrame({
                                'extracted_text': [extracted_text],
                                'source_file': [filename],
                                'confidence_score': [ocr_analysis.get('confidence_score', 0)],
                                'processing_method': [ocr_analysis.get('processing_method', 'ocr')]
                            })
                            return text_df
                    
                    self.logger.warning(f"‚ö†Ô∏è No OCR results found for image: {filename}")
                    return None
                    
                except Exception as ocr_error:
                    self.logger.error(f"‚ùå OCR processing error for {filename}: {ocr_error}")
                    return None
            
            self.logger.warning(f"‚ùå Unsupported file type: {filename}")
            return None
        except Exception as e:
            self.logger.error(f"‚ùå File reading error: {e}")
            return None

    def _generate_fallback_response(self, message: str, language: str) -> str:
        """Generate fallback response when AI is not available"""
        if language == "Turkish":
            return """‚ö†Ô∏è AI servisim ≈üu anda aktif deƒüil, ancak DataSoph AI yetenekleri:

üìä **Veri Analizi:**
‚Ä¢ Otomatik EDA ve veri kalitesi deƒüerlendirmesi
‚Ä¢ ƒ∞statistiksel analiz ve hipotez testleri
‚Ä¢ Geli≈ümi≈ü g√∂rselle≈ütirmeler

ü§ñ **Makine √ñƒürenmesi:**
‚Ä¢ Otomatik model se√ßimi ve hyperparameter tuning
‚Ä¢ Cross-validation ve performance evaluation
‚Ä¢ Feature engineering √∂nerileri

üíº **ƒ∞≈ü Zekasƒ±:**
‚Ä¢ ROI hesaplamalarƒ± ve i≈ü etkisi analizi
‚Ä¢ Executive summary ve actionable insights

L√ºtfen sorunuzu tekrar sorun veya bir veri dosyasƒ± y√ºkleyin."""
        else:
            return """‚ö†Ô∏è AI service not currently active, but DataSoph AI capabilities include:

üìä **Data Analysis:**
‚Ä¢ Automated EDA and data quality assessment
‚Ä¢ Statistical analysis and hypothesis testing
‚Ä¢ Advanced visualizations

ü§ñ **Machine Learning:**
‚Ä¢ Automatic model selection and hyperparameter tuning
‚Ä¢ Cross-validation and performance evaluation
‚Ä¢ Feature engineering recommendations

üíº **Business Intelligence:**
‚Ä¢ ROI calculations and business impact analysis
‚Ä¢ Executive summaries and actionable insights

Please restate your question or upload a data file."""

    def _generate_fallback_analysis_response(self, file_data: pd.DataFrame, language: str) -> str:
        """Generate fallback analysis when AI is not available"""
        if language == "Turkish":
            return f"""üìä **Veri Seti Analizi (Fallback Modu)**

**Temel ƒ∞statistikler:**
- Satƒ±r sayƒ±sƒ±: {file_data.shape[0]:,}
- S√ºtun sayƒ±sƒ±: {file_data.shape[1]}
- Eksik deƒüer: {file_data.isnull().sum().sum()}
- Sayƒ±sal s√ºtunlar: {len(file_data.select_dtypes(include=[np.number]).columns)}
- Kategorik s√ºtunlar: {len(file_data.select_dtypes(include=['object']).columns)}

**S√ºtunlar:** {', '.join(file_data.columns.tolist())}

Bu dosya ba≈üarƒ±yla y√ºklendi! AI servisi aktif olduƒüunda daha detaylƒ± analiz yapabilirim."""
        else:
            return f"""üìä **Dataset Analysis (Fallback Mode)**

**Basic Statistics:**
- Rows: {file_data.shape[0]:,}
- Columns: {file_data.shape[1]}
- Missing values: {file_data.isnull().sum().sum()}
- Numerical columns: {len(file_data.select_dtypes(include=[np.number]).columns)}
- Categorical columns: {len(file_data.select_dtypes(include=['object']).columns)}

**Columns:** {', '.join(file_data.columns.tolist())}

Your file has been successfully uploaded! I can provide more detailed analysis when AI service is active."""

    async def analyze_file_with_expert_ai(self, file_id: str, user_id: str = "default_user") -> Dict[str, Any]:
        """Run comprehensive expert data scientist analysis on uploaded file"""
        try:
            # Find the file path
            matching_files = [f for f in os.listdir(UPLOAD_DIR) if f.startswith(file_id)]
            if not matching_files:
                return {"error": "File not found", "success": False}
            
            file_path = os.path.join(UPLOAD_DIR, matching_files[0])
            
            self.logger.info(f"üî¨ Starting Expert Data Scientist Analysis for: {file_path}")
            
            # Run comprehensive analysis with expert analyzer
            analysis_results = self.expert_analyzer.comprehensive_analysis(file_path)
            
            if "error" in analysis_results:
                return {"error": analysis_results["error"], "success": False}
            
            # Generate expert conversation response
            expert_response = self.expert_conversation_ai.generate_expert_response(
                "Provide comprehensive analysis insights", 
                analysis_results
            )
            
            # Mark file as analyzed
            self.file_manager.mark_file_analyzed(user_id, file_id)
            
            return {
                "success": True,
                "expert_analysis": analysis_results,
                "expert_insights": expert_response,
                "analysis_timestamp": datetime.now().isoformat(),
                "file_id": file_id
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Expert analysis error: {e}")
            return {"error": str(e), "success": False}

# Initialize the AI systems - OpenRouter only
enhanced_ai = ComprehensiveDataScienceAI(
    openrouter_api_key=os.getenv("OPENROUTER_API_KEY")
)

# Initialize the new comprehensive DataSoph AI system
datasoph_ai = DataSophAI()

# Initialize safety components
pii_detector = PIIDetector()
statistical_guardrails = StatisticalGuardrails()
error_resilience = ErrorResilienceManager()

# Initialize Smart Response Generator
# Use global instance from smart_response_generator module
smart_responder = smart_response_generator

@app.post("/api/v1/ai/chat", response_model=ChatResponse)
async def chat_with_ai(request: ChatRequest):
    """Enhanced chat that ACTUALLY uses the smart multilingual system"""
    
    # üî• DEBUG: Fonksiyonun √ßaƒürƒ±ldƒ±ƒüƒ±nƒ± kontrol et
    logger.info(f"üéØ SMART AI ENDPOINT CALLED! Message: {request.message[:50]}...")
    
    try:
        # üöÄ FIXED: PROPERLY AWAIT THE ASYNC FUNCTION!
        logger.info(f"üß† Using SmartResponseGenerator for: {request.message}")
        response = await smart_responder.generate_response(
            message=request.message,
            user_id=request.user_id or "default"
        )
        logger.info(f"‚úÖ Smart AI response generated: {response[:100]}...")
        
        # Enhanced response data
        return ChatResponse(
            response=response,
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"‚ùå Smart AI error: {e}")
        
        # Multilingual error handling based on message content
        is_turkish = any(char in '√ßƒüƒ±√∂≈ü√º√áƒûI√ñ≈û√ú' for char in request.message)
        
        error_msg = (
            f"√úzg√ºn√ºm, bir sorun ya≈üadƒ±m: {str(e)}. Tekrar dener misin? üîß" if is_turkish
            else f"Sorry, I encountered an issue: {str(e)}. Could you try again? üîß"
        )
        
        return ChatResponse(
            response=error_msg,
            timestamp=datetime.now().isoformat()
        )

@app.post("/api/v1/ai/reset-conversation")
async def reset_conversation(request: ChatRequest):
    """Reset conversation for fresh start"""
    try:
        result = smart_responder.reset_conversation(request.user_id or "default")
        
        return {
            'success': result['success'],
            'message': result['message'],
            'user_id': result['user_id'],
            'cleared_exchanges': result.get('cleared_exchanges', 0),
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Reset conversation error: {e}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

@app.post("/api/v1/ai/test-language")
async def test_language(request: ChatRequest):
    """Test language detection"""
    try:
        result = smart_responder.test_language_detection(request.message)
        
        return {
            'success': True,
            'detection_result': result,
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Language test error: {e}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

@app.get("/api/v1/ai/conversation-stats/{user_id}")
async def get_conversation_stats(user_id: str):
    """Get conversation statistics for user"""
    try:
        stats = smart_responder.get_conversation_stats(user_id)
        
        return {
            'success': True,
            'stats': stats,
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Conversation stats error: {e}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

@app.get("/api/v1/health")
async def health_check():
    """Health check"""
    return {"status": "healthy", "service": "DataSoph AI - Enhanced Multilingual System"}

@app.get("/api/v1/supported-formats")
async def get_supported_formats():
    """Get comprehensive list of supported file formats"""
    try:
        supported_formats = enhanced_ai.universal_file_handler.get_supported_formats()
        return {
            "status": "success",
            "formats": supported_formats,
            "total_formats": sum(len(category['formats']) if 'formats' in category else 0 
                               for category in supported_formats.values()),
            "message": "DataSoph AI supports comprehensive file format analysis"
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting supported formats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/upload", response_model=UploadResponse)
async def upload_file(file: UploadFile = File(...)):
    """Upload and store file with enhanced format support"""
    try:
        # Generate unique file ID
        file_id = str(uuid.uuid4())
        file_extension = file.filename.split('.')[-1] if '.' in file.filename else 'txt'
        stored_filename = f"{file_id}_{file.filename}"
        file_path = os.path.join(UPLOAD_DIR, stored_filename)
        
        # Save uploaded file
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # Quick format validation using UniversalFileHandler
        try:
            # Just check if format is supported without full processing
            file_ext = ('.' + file_extension).lower()
            is_supported = file_ext in enhanced_ai.universal_file_handler.supported_formats
            
            if is_supported:
                priority = enhanced_ai.universal_file_handler._get_format_priority(file_ext)
                success_message = enhanced_ai.universal_file_handler.upload_messages['supported'].format(
                    file_type=file_extension.upper()
                )
            else:
                success_message = enhanced_ai.universal_file_handler.upload_messages['partially_supported'].format(
                    file_type=file_extension.upper()
                )
                
        except Exception as validation_error:
            logger.warning(f"‚ö†Ô∏è Format validation error: {validation_error}")
            success_message = f"‚úÖ File '{file.filename}' uploaded successfully. I'll do my best to analyze this format."
        
        # Register file for user session (using default user for now)
        user_id = "web_user"  # Could be passed as parameter in future
        enhanced_ai.file_manager.register_file_for_user(user_id, file_id, file.filename)
        
        # CLEAR OLD CONVERSATION CONTEXT: Reset AI conversation history to prevent confusion
        try:
            # Clear smart response generator conversation history for fresh context
            if hasattr(smart_responder, 'reset_conversation'):
                smart_responder.reset_conversation(user_id)
            
            # Clear any old data context from DataSoph AI
            if hasattr(enhanced_ai, 'start_new_session'):
                enhanced_ai.start_new_session(user_id)
            
            logger.info(f"üîÑ Cleared conversation history for user {user_id} - Fresh data context established")
        except Exception as clear_error:
            logger.warning(f"‚ö†Ô∏è Could not clear old context: {clear_error} - continuing with upload")
        
        # NO AUTOMATIC CHARTS - Let AI create them on demand based on user questions
        logger.info(f"üìÅ File ready for intelligent AI analysis: {file.filename}")
        success_message += f"\n\nüß† **Intelligent AI Analysis Ready!**\n\nüìä Your data is now loaded and ready for smart analysis. I'll analyze your data in real-time based on your specific questions.\n\nüí° **Ask me anything about your data:**\n‚Ä¢ \"Verimdeki ana trendler neler?\" \n‚Ä¢ \"En g√º√ßl√º korelasyonlarƒ± g√∂ster\"\n‚Ä¢ \"Anomali tespiti yap\"\n‚Ä¢ \"Makine √∂ƒürenmesi modeli √∂ner\"\n‚Ä¢ \"ƒ∞≈ü i√ßin hangi √∂nerileriniz var?\"\n‚Ä¢ \"Python kodu ile analiz yap\"\n\nHer sorunuz i√ßin √∂zel analiz ve g√∂rselle≈ütirme yapacaƒüƒ±m! üéØ"
        
        # NO AUTOMATIC EXPERT ANALYSIS - AI will handle everything on-demand
        logger.info(f"üìä File ready for intelligent AI conversation: {file.filename}")
        # All analysis will be done by AI when user asks specific questions
        
        logger.info(f"üìÅ File uploaded: {file.filename} -> {file_id} (format: {file_extension})")
        
        return UploadResponse(
            file_id=file_id,
            filename=file.filename,
            message=success_message,
            size=file.size,
            type=file.content_type
        )
        
    except Exception as e:
        logger.error(f"‚ùå Upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@app.post("/api/v1/analyze-file")
async def analyze_file_immediately(request: FileAnalysisRequest):
    """Trigger immediate comprehensive analysis of uploaded file using UniversalFileHandler"""
    try:
        user_id = request.user_id or "web_user"
        
        # Find the file path
        matching_files = [f for f in os.listdir(UPLOAD_DIR) if f.startswith(request.file_id)]
        if not matching_files:
            raise HTTPException(status_code=404, detail="File not found")
        
        file_path = os.path.join(UPLOAD_DIR, matching_files[0])
        
        # Use UniversalFileHandler for comprehensive analysis
        logger.info(f"üî¨ Starting universal file analysis for: {file_path}")
        processing_result = enhanced_ai.universal_file_handler.detect_and_process(file_path)
        
        # Generate AI-powered analysis response
        if processing_result.get('success', True):
            # Create analysis summary for AI
            analysis_summary = f"""üìä **COMPREHENSIVE FILE ANALYSIS RESULTS:**

**File Information:**
- Filename: {processing_result['file_info'].get('filename', 'Unknown')}
- Size: {processing_result['file_info'].get('size_mb', 0):.2f} MB
- Format: {processing_result['processing_result'].get('format', 'Unknown')}
- Processing Status: {'‚úÖ Success' if processing_result['processing_result'].get('success', False) else '‚ùå Failed'}

**Processing Details:**
{str(processing_result['processing_result'])[:2000]}

Please provide a comprehensive analysis and actionable insights based on this file processing result."""
            
            # Get AI-powered insights
            analysis_result = await enhanced_ai.analyze_file_completely(request.file_id, user_id)
            
            # Combine technical analysis with AI insights
            combined_response = f"""{analysis_result}

---

**üîß Technical Processing Details:**
- **Format Priority:** {processing_result.get('format_priority', 'Unknown')}
- **Processor Version:** {processing_result.get('processor_version', '3.0')}
- **Analysis Timestamp:** {processing_result.get('analysis_timestamp', datetime.now().isoformat())}

{processing_result['processing_result'].get('user_message', '')}"""
            
        else:
            # Handle processing errors
            error_message = processing_result.get('error', 'Unknown processing error')
            combined_response = f"""‚ùå **File Processing Error:**

{error_message}

**What you can try:**
‚Ä¢ Check if your file format is supported
‚Ä¢ Ensure the file isn't corrupted
‚Ä¢ Try converting to a standard format (CSV, Excel, JSON)
‚Ä¢ Contact support if the issue persists

**Supported formats include:** CSV, Excel, JSON, PDF, images, statistical software files, and many more."""

        # Mark file as analyzed
        enhanced_ai.file_manager.mark_file_analyzed(user_id, request.file_id)
        
        return ChatResponse(
            response=combined_response,
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"‚ùå Universal file analysis error: {e}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.post("/api/v1/expert-analysis")
async def expert_data_scientist_analysis(request: FileAnalysisRequest):
    """Run comprehensive expert data scientist analysis with advanced insights"""
    try:
        user_id = request.user_id or "web_user"
        
        # Run expert analysis
        logger.info(f"üî¨ Starting Expert Data Scientist Analysis for file: {request.file_id}")
        expert_results = await enhanced_ai.analyze_file_with_expert_ai(request.file_id, user_id)
        
        if not expert_results.get("success", False):
            raise HTTPException(status_code=400, detail=expert_results.get("error", "Analysis failed"))
        
        # Format comprehensive response
        analysis = expert_results["expert_analysis"]
        insights = expert_results["expert_insights"]
        
        # Build detailed response
        response_parts = ["üî¨ **EXPERT DATA SCIENTIST ANALYSIS COMPLETE**\n"]
        
        # Basic info
        if "basic_info" in analysis:
            basic = analysis["basic_info"]
            response_parts.append(f"üìä **Dataset Overview:**")
            response_parts.append(f"‚Ä¢ Shape: {basic['shape'][0]:,} rows √ó {basic['shape'][1]} columns")
            response_parts.append(f"‚Ä¢ Memory: {basic['memory_mb']:.2f} MB")
            response_parts.append("")
        
        # Data quality
        if "data_quality" in analysis:
            quality = analysis["data_quality"]
            score = quality.get("quality_score", 0)
            emoji = "üü¢" if score > 80 else "üü°" if score > 60 else "üî¥"
            response_parts.append(f"üîç **Data Quality Assessment:**")
            response_parts.append(f"‚Ä¢ Overall Score: {emoji} {score:.1f}/100")
            response_parts.append(f"‚Ä¢ Missing value columns: {quality.get('missing_columns', 0)}")
            response_parts.append(f"‚Ä¢ Duplicate rows: {quality.get('duplicates', 0):,}")
            response_parts.append("")
        
        # Statistical insights
        if "statistical_analysis" in analysis:
            stats = analysis["statistical_analysis"]
            if "correlations" in stats and "high_correlations" in stats["correlations"]:
                high_corrs = stats["correlations"]["high_correlations"]
                if high_corrs:
                    response_parts.append(f"üìà **Key Correlations Found:**")
                    for corr in high_corrs[:3]:
                        direction = "üìà" if corr["correlation"] > 0 else "üìâ"
                        response_parts.append(f"‚Ä¢ {direction} {corr['var1']} ‚Üî {corr['var2']}: {corr['correlation']:.3f}")
                    response_parts.append("")
        
        # Predictive modeling
        if "predictive_modeling" in analysis:
            modeling = analysis["predictive_modeling"]
            model_type = modeling.get("model_type", "unknown")
            response_parts.append(f"ü§ñ **Predictive Model Results:**")
            response_parts.append(f"‚Ä¢ Model Type: {model_type.title()}")
            response_parts.append(f"‚Ä¢ Target Variable: {modeling.get('target_variable', 'N/A')}")
            
            if model_type == "classification":
                acc = modeling.get("accuracy", 0)
                response_parts.append(f"‚Ä¢ Accuracy: {acc:.1%}")
            elif model_type == "regression":
                r2 = modeling.get("r2_score", 0)
                response_parts.append(f"‚Ä¢ R¬≤ Score: {r2:.3f}")
            
            if "feature_importance" in modeling:
                top_feature = max(modeling["feature_importance"], key=modeling["feature_importance"].get)
                response_parts.append(f"‚Ä¢ Most Important Feature: {top_feature}")
            response_parts.append("")
        
        # Business insights
        if "business_insights" in analysis:
            insights_list = analysis["business_insights"]
            response_parts.append("üí° **Business Insights & Recommendations:**")
            for i, insight in enumerate(insights_list[:5], 1):
                response_parts.append(f"{i}. {insight}")
            response_parts.append("")
        
        # Expert conversation insights
        response_parts.append("üéØ **Expert AI Insights:**")
        response_parts.append(insights)
        
        # Visualizations note
        if "visualizations" in analysis:
            viz_count = len(analysis["visualizations"])
            response_parts.append(f"\nüìä **Generated {viz_count} Professional Visualizations:**")
            viz_types = list(analysis["visualizations"].keys())
            for viz_type in viz_types:
                response_parts.append(f"‚Ä¢ {viz_type.replace('_', ' ').title()}")
        
        combined_response = "\n".join(response_parts)
        
        return ChatResponse(
            response=combined_response,
            timestamp=expert_results["analysis_timestamp"]
        )
        
    except Exception as e:
        logger.error(f"‚ùå Expert analysis error: {e}")
        raise HTTPException(status_code=500, detail=f"Expert analysis failed: {str(e)}")

# NEW COMPREHENSIVE DATASOPH AI ENDPOINTS

@app.post("/api/v2/datasoph/chat")
async def datasoph_chat_endpoint(request: ChatRequest):
    """Enhanced DataSoph AI chat endpoint with full feature set"""
    try:
        response = datasoph_ai.process_user_input(
            user_input=request.message,
            file_path=None,
            user_id=request.user_id
        )
        
        return ChatResponse(
            response=response['response'],
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"‚ùå DataSoph chat error: {e}")
        return ChatResponse(
            response=error_resilience.safe_execute_analysis_step(
                lambda: "I encountered an issue. Please try rephrasing your question.",
                str(e)
            )['result'],
            timestamp=datetime.now().isoformat()
        )

@app.post("/api/v2/datasoph/analyze-file")
async def datasoph_file_analysis(file: UploadFile = File(...)):
    """Comprehensive file analysis with DataSoph AI"""
    try:
        # Save uploaded file temporarily
        file_id = str(uuid.uuid4())
        file_extension = file.filename.split('.')[-1] if '.' in file.filename else 'txt'
        stored_filename = f"{file_id}_{file.filename}"
        file_path = os.path.join(UPLOAD_DIR, stored_filename)
        
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # Run comprehensive analysis with DataSoph AI
        result = datasoph_ai.process_user_input(
            user_input="Analyze this file comprehensively",
            file_path=file_path,
            user_id="web_user"
        )
        
        return {
            "analysis_result": result,
            "file_id": file_id,
            "filename": file.filename,
            "status": "completed" if result['success'] else "failed"
        }
        
    except Exception as e:
        logger.error(f"‚ùå DataSoph file analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v2/datasoph/status")
async def get_datasoph_status():
    """Get comprehensive DataSoph AI system status"""
    try:
        status = datasoph_ai.get_system_status()
        
        # Add safety system status
        status['safety_systems'] = {
            'pii_detector': 'ready',
            'statistical_guardrails': 'ready',
            'error_resilience': 'ready',
            'error_summary': error_resilience.get_error_summary()
        }
        
        return status
        
    except Exception as e:
        logger.error(f"‚ùå Status check error: {e}")
        return {"status": "error", "message": str(e)}

@app.post("/api/v2/datasoph/pii-scan")
async def scan_for_pii(request: FileAnalysisRequest):
    """Scan uploaded file for PII and privacy concerns"""
    try:
        # Find the file
        matching_files = [f for f in os.listdir(UPLOAD_DIR) if f.startswith(request.file_id)]
        if not matching_files:
            raise HTTPException(status_code=404, detail="File not found")
        
        file_path = os.path.join(UPLOAD_DIR, matching_files[0])
        
        # Load data and scan for PII
        df = pd.read_csv(file_path)  # Basic loading for PII scan
        pii_results = pii_detector.scan_dataset_for_pii(df)
        
        return {
            "pii_scan_results": pii_results,
            "file_id": request.file_id,
            "scan_timestamp": datetime.now().isoformat(),
            "recommendations": pii_results.get('masking_suggestions', {})
        }
        
    except Exception as e:
        logger.error(f"‚ùå PII scan error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v2/datasoph/export-session")
async def export_session_analysis():
    """Export current session analysis"""
    try:
        export_result = datasoph_ai.export_session_analysis(format='notebook')
        
        return {
            "export_result": export_result,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Export error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v2/datasoph/start-session")
async def start_datasoph_session(user_id: str = "web_user"):
    """Start new DataSoph AI session"""
    try:
        session_result = datasoph_ai.start_new_session(user_id)
        
        return {
            "session": session_result,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Session start error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/ai/clear-data-context")
async def clear_data_context(request: ChatRequest):
    """Clear all data context and conversation history for fresh analysis"""
    try:
        user_id = request.user_id or "web_user"
        
        # Clear conversation history from smart responder
        conversation_result = smart_responder.reset_conversation(user_id)
        
        # Clear file context from file manager
        if user_id in enhanced_ai.file_manager.user_file_sessions:
            old_files = len(enhanced_ai.file_manager.user_file_sessions[user_id])
            enhanced_ai.file_manager.user_file_sessions[user_id] = []
            logger.info(f"üóëÔ∏è Cleared {old_files} file(s) from context for user {user_id}")
        else:
            old_files = 0
        
        # Start fresh DataSoph AI session
        try:
            if hasattr(enhanced_ai, 'start_new_session'):
                enhanced_ai.start_new_session(user_id)
        except Exception as session_error:
            logger.warning(f"‚ö†Ô∏è Could not reset DataSoph session: {session_error}")
        
        # Detect language for response
        is_turkish = any(char in '√ßƒüƒ±√∂≈ü√º√áƒûI√ñ≈û√ú' for char in request.message) if request.message else False
        
        clear_message = (
            f"‚ú® T√ºm veri baƒülamƒ± temizlendi! ({conversation_result.get('cleared_exchanges', 0)} konu≈üma + {old_files} dosya) "
            "≈ûimdi yeni verileriniz i√ßin tamamen temiz bir ba≈ülangƒ±√ß yapabilirsiniz. üîÑ" 
            if is_turkish else 
            f"‚ú® All data context cleared! ({conversation_result.get('cleared_exchanges', 0)} conversations + {old_files} files) "
            "You now have a completely fresh start for your new data. üîÑ"
        )
        
        return {
            'success': True,
            'message': clear_message,
            'user_id': user_id,
            'cleared_conversations': conversation_result.get('cleared_exchanges', 0),
            'cleared_files': old_files,
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Clear data context error: {e}")
        
        error_message = (
            f"Veri baƒülamƒ± temizlenirken hata olu≈ütu: {str(e)}" if '√ßƒüƒ±√∂≈ü√º' in str(e).lower() 
            else f"Error clearing data context: {str(e)}"
        )
        
        return {
            'success': False,
            'message': error_message,
            'timestamp': datetime.now().isoformat()
        }

if __name__ == "__main__":
    logger.info("üöÄ Starting Enhanced DataSoph AI")
    uvicorn.run(
        "main:app",
        host="0.0.0.0", 
        port=8000,
        reload=False,
        log_level="info"
    ) 